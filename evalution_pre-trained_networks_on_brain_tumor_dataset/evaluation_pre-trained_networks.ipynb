{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "chief-pleasure",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np \n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import os\n",
    "import shutil\n",
    "import itertools\n",
    "import imutils\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "from plotly import tools\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.applications.densenet import DenseNet201\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.mobilenet_v2 import MobileNetV2\n",
    "from keras import layers\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "RANDOM_SEED = 123"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moderate-april",
   "metadata": {},
   "source": [
    "# Data Import and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reduced-display",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_PATH = 'brain_tumor_dataset/'\n",
    "for CLASS in os.listdir(IMG_PATH):\n",
    "    if not CLASS.startswith('.'):\n",
    "        IMG_NUM = len(os.listdir(IMG_PATH + CLASS))\n",
    "        for (n, FILE_NAME) in enumerate(os.listdir(IMG_PATH + CLASS)):\n",
    "            img = IMG_PATH + CLASS + '/' + FILE_NAME\n",
    "            if n < 5:\n",
    "                shutil.copy(img, 'TEST/' + CLASS.upper() + '/' + FILE_NAME)\n",
    "            elif n < 0.8*IMG_NUM:\n",
    "                shutil.copy(img, 'TRAIN/' + CLASS.upper() + '/' + FILE_NAME)\n",
    "            else:\n",
    "                shutil.copy(img, 'VAL/' + CLASS.upper() + '/' + FILE_NAME) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "appropriate-tunisia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dir_path, img_size=(100, 100)):\n",
    "    X = []\n",
    "    y = []\n",
    "    i = 0\n",
    "    labels = dict()\n",
    "    for path in tqdm(sorted(os.listdir(dir_path))):\n",
    "        if not path.startswith('.'):\n",
    "            labels[i] = path\n",
    "            for file in os.listdir(dir_path + path):\n",
    "                if not file.startswith('.'):\n",
    "                    img = cv2.imread(dir_path + path + '/' + file)\n",
    "                    X.append(img)\n",
    "                    y.append(i)\n",
    "            i += 1\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    print('{} images loaded from {} directory.'.format(len(X), dir_path))\n",
    "    return X, y, labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "distant-creator",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  2.80it/s]\n",
      "/home/zak/anaconda3/envs/cv/lib/python3.6/site-packages/ipykernel_launcher.py:15: VisibleDeprecationWarning:\n",
      "\n",
      "Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "\n",
      "100%|██████████| 2/2 [00:00<00:00, 31.33it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193 images loaded from TRAIN/ directory.\n",
      "10 images loaded from TEST/ directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  9.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 images loaded from VAL/ directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DIR = 'TRAIN/'\n",
    "TEST_DIR = 'TEST/'\n",
    "VAL_DIR = 'VAL/'\n",
    "IMG_SIZE = (224, 224)\n",
    "\n",
    "X_train, y_train, labels = load_data(TRAIN_DIR, IMG_SIZE)\n",
    "X_test, y_test, _ = load_data(TEST_DIR, IMG_SIZE)\n",
    "X_val, y_val, _ = load_data(VAL_DIR, IMG_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "another-adrian",
   "metadata": {},
   "source": [
    "# CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "after-windows",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "equal-sample",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 193 images belonging to 2 classes.\n",
      "Found 50 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    brightness_range=[0.5, 1.5],\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    color_mode='rgb',\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    seed=RANDOM_SEED\n",
    ")\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    VAL_DIR,\n",
    "    color_mode='rgb',\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=16,\n",
    "    class_mode='binary',\n",
    "    seed=RANDOM_SEED\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "balanced-senate",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "exotic-rental",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 5s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# loading base model\n",
    "base_model_vgg = VGG16(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=IMG_SIZE + (3,)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "plain-peninsula",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 7, 7, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 25089     \n",
      "=================================================================\n",
      "Total params: 14,739,777\n",
      "Trainable params: 25,089\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = 1\n",
    "\n",
    "model_vgg = Sequential()\n",
    "model_vgg.add(base_model_vgg)\n",
    "model_vgg.add(layers.Flatten())\n",
    "model_vgg.add(layers.Dropout(0.5))\n",
    "model_vgg.add(layers.Dense(NUM_CLASSES, activation='sigmoid'))\n",
    "\n",
    "model_vgg.layers[0].trainable = False\n",
    "\n",
    "model_vgg.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=RMSprop(lr=1e-4),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model_vgg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "palestinian-length",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "7/7 [==============================] - 166s 24s/step - loss: 3.9283 - accuracy: 0.5751 - val_loss: 0.6458 - val_accuracy: 0.6400\n",
      "Epoch 2/30\n",
      "7/7 [==============================] - 147s 21s/step - loss: 3.2722 - accuracy: 0.6477 - val_loss: 0.0067 - val_accuracy: 0.7400\n",
      "Epoch 3/30\n",
      "7/7 [==============================] - 145s 21s/step - loss: 3.4294 - accuracy: 0.6632 - val_loss: 9.6659 - val_accuracy: 0.7400\n",
      "Epoch 4/30\n",
      "7/7 [==============================] - 142s 20s/step - loss: 3.4174 - accuracy: 0.6477 - val_loss: 1.4265e-04 - val_accuracy: 0.7600\n",
      "Epoch 5/30\n",
      "7/7 [==============================] - 115s 16s/step - loss: 2.5148 - accuracy: 0.7358 - val_loss: 8.4035 - val_accuracy: 0.7600\n",
      "Epoch 6/30\n",
      "7/7 [==============================] - 95s 14s/step - loss: 3.2295 - accuracy: 0.7047 - val_loss: 5.3857e-06 - val_accuracy: 0.7800\n",
      "Epoch 7/30\n",
      "7/7 [==============================] - 93s 13s/step - loss: 2.5599 - accuracy: 0.6788 - val_loss: 0.0063 - val_accuracy: 0.7800\n",
      "Epoch 8/30\n",
      "7/7 [==============================] - 92s 13s/step - loss: 3.2132 - accuracy: 0.6943 - val_loss: 1.8709e-04 - val_accuracy: 0.7600\n",
      "Epoch 9/30\n",
      "7/7 [==============================] - 89s 13s/step - loss: 1.7907 - accuracy: 0.7824 - val_loss: 7.6640e-07 - val_accuracy: 0.7800\n",
      "Epoch 10/30\n",
      "7/7 [==============================] - 91s 13s/step - loss: 2.1938 - accuracy: 0.7202 - val_loss: 6.1818e-05 - val_accuracy: 0.7800\n",
      "Epoch 11/30\n",
      "7/7 [==============================] - 106s 15s/step - loss: 1.7667 - accuracy: 0.7306 - val_loss: 7.8512e-05 - val_accuracy: 0.7600\n",
      "Epoch 12/30\n",
      "7/7 [==============================] - 109s 16s/step - loss: 2.3331 - accuracy: 0.7358 - val_loss: 16.4836 - val_accuracy: 0.7600\n",
      "Epoch 13/30\n",
      "7/7 [==============================] - 138s 20s/step - loss: 1.6845 - accuracy: 0.7565 - val_loss: 1.7001e-08 - val_accuracy: 0.7600\n",
      "Epoch 14/30\n",
      "7/7 [==============================] - 91s 13s/step - loss: 1.5347 - accuracy: 0.7617 - val_loss: 0.7981 - val_accuracy: 0.8000\n",
      "Epoch 15/30\n",
      "7/7 [==============================] - 89s 13s/step - loss: 1.5290 - accuracy: 0.7824 - val_loss: 2.0687 - val_accuracy: 0.8000\n",
      "Epoch 16/30\n",
      "7/7 [==============================] - 106s 15s/step - loss: 3.8766 - accuracy: 0.7720 - val_loss: 2.3662 - val_accuracy: 0.7800\n",
      "Epoch 17/30\n",
      "7/7 [==============================] - 106s 15s/step - loss: 1.3277 - accuracy: 0.8135 - val_loss: 1.0549 - val_accuracy: 0.8000\n",
      "Epoch 18/30\n",
      "7/7 [==============================] - 140s 20s/step - loss: 1.6577 - accuracy: 0.7876 - val_loss: 1.9458e-11 - val_accuracy: 0.8000\n",
      "Epoch 19/30\n",
      "7/7 [==============================] - 137s 20s/step - loss: 1.8831 - accuracy: 0.8238 - val_loss: 2.7784e-09 - val_accuracy: 0.8200\n",
      "Epoch 20/30\n",
      "7/7 [==============================] - 82s 12s/step - loss: 1.6757 - accuracy: 0.7927 - val_loss: 4.3050 - val_accuracy: 0.8200\n",
      "Epoch 21/30\n",
      "7/7 [==============================] - 91s 13s/step - loss: 1.4432 - accuracy: 0.8135 - val_loss: 5.1555 - val_accuracy: 0.8400\n",
      "Epoch 22/30\n",
      "7/7 [==============================] - 82s 12s/step - loss: 1.5488 - accuracy: 0.8083 - val_loss: 0.4623 - val_accuracy: 0.8200\n",
      "Epoch 23/30\n",
      "7/7 [==============================] - 91s 13s/step - loss: 1.6085 - accuracy: 0.8187 - val_loss: 4.7952 - val_accuracy: 0.8400\n",
      "Epoch 24/30\n",
      "7/7 [==============================] - 82s 12s/step - loss: 3.9222 - accuracy: 0.7668 - val_loss: 8.9315e-09 - val_accuracy: 0.8600\n",
      "Epoch 25/30\n",
      "7/7 [==============================] - 81s 12s/step - loss: 1.7606 - accuracy: 0.8083 - val_loss: 3.4961 - val_accuracy: 0.8600\n",
      "Epoch 26/30\n",
      "7/7 [==============================] - 81s 12s/step - loss: 1.3882 - accuracy: 0.7927 - val_loss: 5.0995 - val_accuracy: 0.8600\n",
      "Epoch 27/30\n",
      "7/7 [==============================] - 84s 12s/step - loss: 1.1411 - accuracy: 0.8394 - val_loss: 8.9009e-14 - val_accuracy: 0.8800\n",
      "Epoch 28/30\n",
      "7/7 [==============================] - 81s 12s/step - loss: 0.9989 - accuracy: 0.8446 - val_loss: 3.5886 - val_accuracy: 0.8600\n",
      "Epoch 29/30\n",
      "7/7 [==============================] - 88s 13s/step - loss: 1.6756 - accuracy: 0.8290 - val_loss: 6.5157 - val_accuracy: 0.8200\n",
      "Epoch 30/30\n",
      "7/7 [==============================] - 86s 12s/step - loss: 1.5698 - accuracy: 0.8653 - val_loss: 0.0064 - val_accuracy: 0.8400\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 30\n",
    "\n",
    "history_vgg = model_vgg.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=len(validation_generator)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convinced-seeker",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
