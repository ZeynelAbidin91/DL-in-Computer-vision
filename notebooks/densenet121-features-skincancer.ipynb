{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XvBDTao8djYJ"
   },
   "source": [
    "# Melanoma Classification\n",
    "\n",
    "Kaggle Competition Page: www.kaggle.com/c/siim-isic-melanoma-classification/overview\n",
    "\n",
    "\n",
    "## What is Melanoma?\n",
    "Melanoma, the most severe type of skin cancer, develops in the cells (melanocytes) that produce melanin — the pigment that gives your skin its color. Melanoma can also form in your eyes and, rarely, inside your body, such as in your nose or throat.\n",
    "\n",
    "The exact cause of all melanomas isn't clear, but exposure to ultraviolet (UV) radiation from sunlight or tanning lamps and beds increases your risk of developing melanoma.\n",
    "\n",
    "The risk of melanoma seems to be increasing in people under 40, especially women. Knowing the warning signs of skin cancer can help ensure that cancerous changes are detected and treated before the cancer has spread. We can treat melanoma successfully if it is detected early."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQIr6OLjdjYs"
   },
   "source": [
    "<img src=\"https://github.com/SaschaMet/melanoma-classification/blob/master/images/melanoma.jpg?raw=1\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wR5V4LGwdjYs"
   },
   "source": [
    "## Symptoms & Diagnosis\n",
    "Melanomas can develop anywhere on your body. They most often develop in areas with exposure to the sun, such as your back, legs, arms, and face.\n",
    "Melanomas can also occur in areas that don't receive much sun exposure, such as the soles of your feet, palms of your hands, and fingernail beds. These hidden melanomas are more common in people with darker skin.\n",
    "\n",
    "To help you identify characteristics of melanomas or other skin cancers, think of the letters ABCDE:\n",
    "- A is for asymmetrical shape. Look for moles with irregular shapes, such as two very different-looking halves.\n",
    "- B is for irregular border. Look for moles with rough, notched, or scalloped edges — characteristics of melanomas.\n",
    "- C is for color changes. Look for growths that have many colors or an uneven distribution of color.\n",
    "- D is for diameter. Look for new growth in a mole larger than 1/4 inch (about 6 millimeters).\n",
    "- E is for evolving. Look for changes over time, such as a mole that grows in size or changes color or shape.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_Vl79qwdjYw"
   },
   "source": [
    "![ABCDE Melanoma](https://github.com/SaschaMet/melanoma-classification/blob/master/images/abcde-melanoma.jpg?raw=1)\n",
    "\n",
    "Source: https://www.health.harvard.edu/cancer/melanoma-overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bm8mD1YSdjYx"
   },
   "source": [
    "The facts about Melanoma:\n",
    "- Melanoma is the most severe form of skin cancer\n",
    "- It makes up 2% of skin cancers but is responsible for 75% of skin cancer deaths\n",
    "- Australia and New Zealand have the highest melanoma rates in the world\n",
    "- 1 in 17 Australians will be diagnosed with melanoma before the age of 85\n",
    "- More than 90% of melanoma can be successfully treated with surgery if detected early\n",
    "\n",
    "Source: https://melanomapatients.org.au/about-melanoma/melanoma-facts/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "si5Twho8djYx"
   },
   "source": [
    "<img src=\"https://github.com/SaschaMet/melanoma-classification/blob/master/images/melanoma-impact.jpg?raw=1\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "Source: https://impactmelanoma.org/wp-content/uploads/2018/11/Standard-Infographic_0.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-AFobcxdjYx"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "E06dG0gndjYy"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import warnings\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from datetime import datetime, date\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras.backend as K\n",
    "from pandas_profiling import ProfileReport\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, plot_precision_recall_curve, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "JnvR9SrjdjYy",
    "outputId": "1bc1ff6f-760d-4cff-bfd8-9885ae28ebb7"
   },
   "outputs": [],
   "source": [
    "SEED = 1\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 2\n",
    "VERBOSE_LEVEL = 1\n",
    "SAVE_OUTPUT = True\n",
    "IMG_SIZE = (224, 224)\n",
    "INPUT_SHAPE = (224, 224, 3)\n",
    "\n",
    "CWD = os.getcwd()\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "7QrMRrKjdjYz",
    "outputId": "10df2198-94f3-462f-e656-f1d18fe28bd3"
   },
   "outputs": [],
   "source": [
    "BASE_PATH = '/kaggle/input/siim-isic-melanoma-classification'\n",
    "PATH_TO_IMAGES = '/kaggle/input/siim-isic-melanoma-classification/jpeg' \n",
    "IMAGE_TYPE = \".jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7Vq9jDVdjY3"
   },
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "mvQWw7NtdjY3"
   },
   "outputs": [],
   "source": [
    "\"\"\" Helper function to validate the image paths\n",
    "\n",
    "    Parameters:\n",
    "        file_path (string): Path to the image \n",
    "\n",
    "    Returns:\n",
    "        The file path if the file exists, otherwise false if the file does not exist\n",
    "\n",
    "\"\"\"\n",
    "def check_image(file_path):\n",
    "    img_file = Path(file_path)\n",
    "    if img_file.is_file():\n",
    "        return file_path\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "kA5weeYJdjY4"
   },
   "outputs": [],
   "source": [
    "\"\"\" Helper function to get the train dataset\n",
    "\"\"\"\n",
    "def get_train_data():\n",
    "    # read the data from the train.csv file\n",
    "    train = pd.read_csv(os.path.join(BASE_PATH, 'train.csv'))\n",
    "    # add the image_path to the train set\n",
    "    train['image_path'] = train['image_name'].apply(lambda x: PATH_TO_IMAGES + \"/train/\" + x + IMAGE_TYPE)\n",
    "    # check if the we have an image \n",
    "    train['image_path'] = train.apply(lambda row : check_image(row['image_path']), axis = 1)\n",
    "    # if we do not have an image we will not include the data\n",
    "    train = train[train['image_path'] != False]\n",
    "    print(\"valid rows in train\", train.shape[0])\n",
    "    return train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "kdk3Ha_rdjY4",
    "outputId": "5ccae5ce-4f94-4c08-ab80-b69aee63b62e"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/input/siim-isic-melanoma-classification\\\\train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-ce928f3c6ab0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_train_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-a3ef85e3cd09>\u001b[0m in \u001b[0;36mget_train_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_train_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m# read the data from the train.csv file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBASE_PATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'train.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;31m# add the image_path to the train set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'image_path'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'image_name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mPATH_TO_IMAGES\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/train/\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mIMAGE_TYPE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1048\u001b[0m             )\n\u001b[0;32m   1049\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1051\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1867\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1868\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1366\u001b[0m             \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1368\u001b[1;33m             \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1369\u001b[0m         )\n\u001b[0;32m   1370\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    645\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    646\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 647\u001b[1;33m                 \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    648\u001b[0m             )\n\u001b[0;32m    649\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/siim-isic-melanoma-classification\\\\train.csv'"
     ]
    }
   ],
   "source": [
    "train = get_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SAukZCdpdjY4",
    "outputId": "60aea1cf-1c86-403f-ea29-b727710daea1"
   },
   "outputs": [],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-xS3V7IedjY5"
   },
   "source": [
    "Train Dataset:\n",
    "- image name: the filename for the specific image\n",
    "- patient_id: unique patient id\n",
    "- sex: gender of the patient\n",
    "- age_approx: age of the patient\n",
    "- anatom_site_general_challenge: location of the scan site\n",
    "- diagnosis: information about the diagnosis\n",
    "- benign_malignant: indicates if the scan result is malignant or benign\n",
    "- target: 0 for benign and 1 for malignant\n",
    "- image_path: path to the image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezInl9iUdjZu"
   },
   "source": [
    "### Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C7CSiXIbdjZu",
    "outputId": "fe3c2689-44e2-445b-9ed9-bd2fd581c09c"
   },
   "outputs": [],
   "source": [
    "\"\"\" Helper function check a dataframe for missing values\n",
    "\n",
    "    Parameters:\n",
    "        df (dataframe): The dataframe to check\n",
    "\n",
    "    Returns:\n",
    "        A dataframe with the number of missing and zero values for each column in percent\n",
    "\n",
    "\"\"\"\n",
    "def check_for_missing_and_null(df):\n",
    "    null_df = pd.DataFrame({'columns': df.columns, \n",
    "                            'percent_null': df.isnull().sum() * 100 / len(df), \n",
    "                            'percent_zero': df.isin([0]).sum() * 100 / len(df),\n",
    "                            'total_zero': df.isnull().sum() * 100 / len(df) + df.isin([0]).sum() * 100 / len(df),\n",
    "                           })\n",
    "    return null_df\n",
    "\n",
    "check_for_missing_and_null(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5yaIn2dQdjZu"
   },
   "source": [
    "There is a small portion of missing values for age and sex, as well as for the anatom_site_general_challenge column. \n",
    "\n",
    "The target column consists of 98 % zero values. This means we have a highly imbalanced dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCB_FFkYdjZx"
   },
   "source": [
    "### Removing missing values\n",
    "\n",
    "To do the EDA, I will remove the dataset's missing values because we will not lose much information. Later, when we prepare the dataset for training, I will add these missing values again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TuszpnbgdjZy"
   },
   "outputs": [],
   "source": [
    "train = train.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dpro3gGYdjZy"
   },
   "source": [
    "### Target distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UowDHn02djZy",
    "outputId": "fd60d213-4ecf-4df7-c646-2d7f91273f60"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,6))\n",
    "x = plt.bar([\"Melanoma\",\"Benign\"],[len(train[train.target==1]), len(train[train.target==0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LnZ0oqxvgKL8",
    "outputId": "dc6ceac8-bbe9-4418-cb95-7aa56c1c137f"
   },
   "outputs": [],
   "source": [
    "benign_cases = train[train.target == 0]\n",
    "melanoma_cases = train[train.target == 1]\n",
    "\n",
    "print(\"Benign Cases\", len(benign_cases))\n",
    "print(\"Melanoma Cases\", len(melanoma_cases))\n",
    "print(\" \")\n",
    "print(\"There are only\", len(melanoma_cases), \"malignant cases in the dataset. This is very important to know, because this has implications on how to perpare the dataset for training the machine learning model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "65LkV7UQdjZz"
   },
   "source": [
    "### Gender distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bsBh6hYpdjZz",
    "outputId": "d5d968d4-554e-4c97-85af-b6e4bf631cb8"
   },
   "outputs": [],
   "source": [
    "female = train[train.sex == \"female\"]\n",
    "male = train[train.sex == \"male\"]\n",
    "plt.figure(figsize = (8,6))\n",
    "x = plt.bar(\n",
    "    [\"Female\",\"Male\"],\n",
    "    [len(female), len(male)]\n",
    ")\n",
    "print('There are', len(female), 'female patients in the dataset and', len(male), 'male patients.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A-pyTo-R0_fz",
    "outputId": "7b8e46e2-b651-4235-af94-e0be8e2e9b13"
   },
   "outputs": [],
   "source": [
    "benign_cases_female = train[train.target==0][train.sex == \"female\"]\n",
    "malignant_cases_female = train[train.target==1][train.sex == \"female\"]\n",
    "\n",
    "benign_cases_male = train[train.target==0][train.sex == \"male\"]\n",
    "malignant_cases_male = train[train.target==1][train.sex == \"male\"]\n",
    "\n",
    "plt.figure(figsize = (8,6))\n",
    "x = plt.bar(\n",
    "    [\"Benign & Female\",\"Malignant & Female\", \"Benign & Male\",\"Malignant & Male\"],\n",
    "    [len(benign_cases_female), len(malignant_cases_female), len(benign_cases_male), len(malignant_cases_male)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prEJvzLwdjZ1",
    "outputId": "ab58980f-18ca-42f1-a152-275dce1e745b"
   },
   "outputs": [],
   "source": [
    "grouped_df_by_sex = train.groupby(['target','sex'])['benign_malignant'].count().to_frame().reset_index()\n",
    "grouped_df_by_sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Su2PnvF6ixFI",
    "outputId": "9ac244ed-1308-4b3b-a8c4-9029150205e5"
   },
   "outputs": [],
   "source": [
    "f_m = train[train.target == 1][train.sex == \"female\"]\n",
    "m_m = train[train.target == 1][train.sex == \"male\"]\n",
    "\n",
    "print(\"There are\", len(m_m) ,\"malignant male cases in the dataset compared to\", len(f_m) ,\"female cases.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "noATRLdYdjZ7"
   },
   "source": [
    "### Age distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VJ947xNOdjZ7"
   },
   "outputs": [],
   "source": [
    "# create ten age bins, from 0 to 100\n",
    "age_bins = np.arange(0, 100, 10)\n",
    "\n",
    "\"\"\" Helper function to return the age bin for a specific age\n",
    "\n",
    "    Parameters:\n",
    "        age (int)\n",
    "\n",
    "    Returns:\n",
    "        age bin (int)\n",
    "\"\"\"\n",
    "def add_age_bin(age):\n",
    "    for idx, val in enumerate(age_bins):\n",
    "        if age < val:\n",
    "            return idx\n",
    "\n",
    "# add the age bins to the train df\n",
    "train['age_bin'] = train.apply(lambda row : add_age_bin(row['age_approx']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eDLo_EZYdjZ7",
    "outputId": "1de61ad8-6271-4d42-b302-1c21ed924f9f"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.hist( train.age_bin, bins = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_nkOM4msdjZ8",
    "outputId": "0748bc5a-ba7e-49b7-a9d2-96532f8d4330"
   },
   "outputs": [],
   "source": [
    "print(\"The mean age of a patient in the dataset is\", round(np.mean(train.age_approx, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "836oCQZodjZ8",
    "outputId": "95909d95-a70a-4c8c-9ba0-9feeaf6a9b19"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.hist( train[train.target==1].age_bin, bins = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iXKLO5frdjZ9"
   },
   "source": [
    "The age distributions follows a normal distribution. If we look only at the malignant cases however we can see, that the distribution seems to be wider. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7UBG-TCVdjZ9",
    "outputId": "30240b1d-8462-4ea3-a744-92b065f9dd59"
   },
   "outputs": [],
   "source": [
    "def get_ratio_by_age_bin(age_bin):\n",
    "    total = train[train['age_bin'] == age_bin]\n",
    "    malignant = train[train['age_bin'] == age_bin][train['target'] == 1]\n",
    "    return round((len(malignant) / len(total)) * 100, 2)\n",
    "    \n",
    "for age_bin in [2,3,4,5,6,7,8]:\n",
    "    print(\"Ratio malignant / total cases for age_bin\", age_bin, \"=\" , get_ratio_by_age_bin(age_bin))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6H3jKX8sdjZ-"
   },
   "source": [
    "There are indeed more malignant cases at the ends of the age distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QIsSW6Z7djZ-"
   },
   "source": [
    "### Anatom Site General Challenge distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ubTnh6V-djZ-",
    "outputId": "87831388-f508-444d-b098-e71fb2600f30"
   },
   "outputs": [],
   "source": [
    "anatom_site = list(train.anatom_site_general_challenge.unique())\n",
    "anatom_site = [x for x in anatom_site if str(x) != 'nan']\n",
    "\n",
    "anatom_site_value_counts = []\n",
    "for x in anatom_site:\n",
    "    y = train[train['anatom_site_general_challenge'] == x]\n",
    "    anatom_site_value_counts.append(len(y))\n",
    "\n",
    "y_pos = np.arange(len(anatom_site))\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.bar(y_pos, anatom_site_value_counts, align='center')\n",
    "plt.xticks(y_pos, anatom_site)\n",
    "plt.ylabel('# of rows')\n",
    "plt.title('Anatom Site General Challenge')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most often a lesion was found in the torso area, followed by the lower and upper extremity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8bR7V1xWdjZ_"
   },
   "source": [
    "### Diagnosis distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hdhrURJ0djZ_"
   },
   "outputs": [],
   "source": [
    "diagnosis = list(train.diagnosis.unique())\n",
    "diagnosis = [x for x in diagnosis if str(x) != 'unknown']\n",
    "\n",
    "diagnosis_value_counts = []\n",
    "for x in diagnosis:\n",
    "    y = train[train['diagnosis'] == x]\n",
    "    diagnosis_value_counts.append(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CDNIXujZdjZ_",
    "outputId": "c1b0fe99-7314-4b38-fa86-0b18f64c0399"
   },
   "outputs": [],
   "source": [
    "labels = diagnosis\n",
    "sizes = diagnosis_value_counts\n",
    "plt.figure(figsize=(8,6))\n",
    "patches, texts = plt.pie(sizes, shadow=True, startangle=90)\n",
    "plt.legend(patches, labels, loc=\"best\")\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FW-F5CQIdjZ_"
   },
   "source": [
    "The main finding in the dataset is \"nevus\". Nevus is a nonspecific medical term for a visible, circumscribed, chronic lesion of the skin (e.g. a \"birthmark\"). The second most common finding was melanoma.\n",
    "\n",
    "\n",
    "Source: https://en.wikipedia.org/wiki/Nevus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XrP2muWNdjZ_"
   },
   "source": [
    "## Images from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MBHUu1ITdjaA",
    "outputId": "b9f73fd6-4a13-48ec-d0a5-c50bb5576bf2"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 16))\n",
    "for i in range(9):\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    img_path = train.iloc[i].image_path\n",
    "    img = plt.imread(img_path)\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NTOLu_PxdjaE"
   },
   "source": [
    "## Data preparation\n",
    "\n",
    "Because we removed some values from the dataset for the EDA, we load the train and test set again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DP1lSkATdjaE",
    "outputId": "f2ea5e6c-9ce5-4ced-d522-883f5f7aa5b3"
   },
   "outputs": [],
   "source": [
    "train = get_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tG54suCTdjaF",
    "outputId": "ad62324d-efa0-4ed9-9813-f9b1edc473ab"
   },
   "outputs": [],
   "source": [
    "# getting dummy variables for gender\n",
    "sex_dummies = pd.get_dummies(train['sex'], prefix='sex', dtype=\"int\")\n",
    "train = pd.concat([train, sex_dummies], axis=1)\n",
    "\n",
    "# getting dummy variables for anatom_site_general_challenge\n",
    "anatom_dummies = pd.get_dummies(train['anatom_site_general_challenge'], prefix='anatom', dtype=\"int\")\n",
    "train = pd.concat([train, anatom_dummies], axis=1)\n",
    "\n",
    "# getting dummy variables for target column\n",
    "#target_dummies = pd.get_dummies(train['target'], prefix='target', dtype=\"int\")\n",
    "#train = pd.concat([train, target_dummies], axis=1)\n",
    "\n",
    "# dropping not useful columns\n",
    "train.drop(['sex','diagnosis','benign_malignant','anatom_site_general_challenge'], axis=1, inplace=True)\n",
    "\n",
    "# replace missing age values wiht the mean age\n",
    "train['age_approx'] = train['age_approx'].fillna(int(np.mean(train['age_approx'])))\n",
    "\n",
    "# convert age to int\n",
    "train['age_approx'] = train['age_approx'].astype('int')\n",
    "\n",
    "print(\"rows in train\", train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jBNW9t1MdjaI",
    "outputId": "b18733b1-ae80-4491-bcc9-9f386f3652eb"
   },
   "outputs": [],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bamRcA-xdjaJ"
   },
   "source": [
    "### Balance the dataset\n",
    "\n",
    "Because we have a highly imbalanced dataset we need to balance it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Ayqx5OTdjaJ",
    "outputId": "4deb5bc6-f858-4c91-fb14-9c33f3a05d66"
   },
   "outputs": [],
   "source": [
    "# 1 means 50 / 50 => equal amount of positive and negative cases in Training\n",
    "# 4 = 20%; 8 = ~11%; 12 = ~8%\n",
    "balance = 1\n",
    "p_inds = train[train.target == 1].index.tolist()\n",
    "np_inds = train[train.target == 0].index.tolist()\n",
    "\n",
    "np_sample = random.sample(np_inds, balance * len(p_inds))\n",
    "train = train.loc[p_inds + np_sample]\n",
    "print(\"Samples in train\", train['target'].sum()/len(train))\n",
    "print(\"Remaining rows in train set\", len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bIdtddiJdjaL"
   },
   "outputs": [],
   "source": [
    "\"\"\" Helper function to create a train and a validation dataset\n",
    "\n",
    "    Parameters:\n",
    "    df (dataframe): The dataframe to split\n",
    "    test_size (int): Size of the validation set\n",
    "    classToPredict: The target column\n",
    "\n",
    "    Returns:\n",
    "    train_data (dataframe)\n",
    "    val_data (dataframe)\n",
    "\"\"\"\n",
    "def create_splits(df, test_size, classToPredict):\n",
    "    train_data, val_data = train_test_split(df,  test_size = test_size, random_state = 1, stratify = df[classToPredict])\n",
    "    train_data, test_data = train_test_split(df,  test_size = 0.16, random_state = 1, stratify = df[classToPredict])\n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wdrJ0CbSdjaN"
   },
   "outputs": [],
   "source": [
    "\"\"\" Helper function to plot the history of a tensorflow model\n",
    "\n",
    "    Parameters:\n",
    "        history (history object): The history from a tf model\n",
    "        timestamp (string): The timestamp of the function execution\n",
    "\n",
    "    Returns:\n",
    "        Null\n",
    "\"\"\"\n",
    "def save_history(history, timestamp):\n",
    "    f = plt.figure()\n",
    "    f.set_figwidth(15)\n",
    "\n",
    "    f.add_subplot(1, 2, 1)\n",
    "    plt.plot(history['val_loss'], label='val loss')\n",
    "    plt.plot(history['loss'], label='train loss')\n",
    "    plt.legend()\n",
    "    plt.title(\"Modell Loss\")\n",
    "\n",
    "    f.add_subplot(1, 2, 2)\n",
    "    plt.plot(history['val_accuracy'], label='val accuracy')\n",
    "    plt.plot(history['accuracy'], label='train accuracy')\n",
    "    plt.legend()\n",
    "    plt.title(\"Modell Accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VUFWVx4hdjaN"
   },
   "outputs": [],
   "source": [
    "\"\"\" Helper function to plot the auc curve\n",
    "\n",
    "    Parameters:\n",
    "        t_y (array): True binary labels\n",
    "        p_y (array): Target scores\n",
    "\n",
    "    Returns:\n",
    "        Null\n",
    "\"\"\"\n",
    "def plot_auc(t_y, p_y):\n",
    "    fpr, tpr, thresholds = roc_curve(t_y, p_y, pos_label=1)\n",
    "    fig, c_ax = plt.subplots(1,1, figsize = (8, 8))\n",
    "    c_ax.plot(fpr, tpr, label = '%s (AUC:%0.2f)'  % ('Target', auc(fpr, tpr)))\n",
    "    c_ax.plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--')\n",
    "    c_ax.legend()\n",
    "    c_ax.set_xlabel('False Positive Rate')\n",
    "    c_ax.set_ylabel('True Positive Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0DkseDQSdjaN"
   },
   "outputs": [],
   "source": [
    "\"\"\" Factory function to create a training image data generator\n",
    "\n",
    "Parameters:\n",
    "    df (dataframe): Training dataframe \n",
    "\n",
    "Returns:\n",
    "    Image Data Generator function\n",
    "\"\"\"\n",
    "def get_training_gen(df):\n",
    "    ## prepare images for training\n",
    "    train_idg = ImageDataGenerator(\n",
    "        rescale = 1 / 255.0,\n",
    "        horizontal_flip = True, \n",
    "        vertical_flip = True, \n",
    "        height_shift_range = 0.15, \n",
    "        width_shift_range = 0.15,\n",
    "        shear_range=0.15,\n",
    "        rotation_range = 90, \n",
    "        zoom_range = 0.20,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "\n",
    "    train_gen = train_idg.flow_from_dataframe(\n",
    "        seed=SEED,\n",
    "        dataframe=df,\n",
    "        directory=None,\n",
    "        x_col='image_path',\n",
    "        y_col='target',\n",
    "        class_mode='raw',\n",
    "        shuffle=True,\n",
    "        target_size=IMG_SIZE,\n",
    "        #batch_size=BATCH_SIZE,\n",
    "        validate_filenames = False\n",
    "    )\n",
    "\n",
    "    return train_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IoYfVIj1djaN"
   },
   "outputs": [],
   "source": [
    "\"\"\" Factory function to create a validation image data generator\n",
    "\n",
    "Parameters:\n",
    "    df (dataframe): Validation dataframe \n",
    "\n",
    "Returns:\n",
    "    Image Data Generator function\n",
    "\"\"\"\n",
    "def get_validation_gen(df):\n",
    "    ## prepare images for validation\n",
    "    val_idg = ImageDataGenerator(rescale=1. / 255.0)\n",
    "    val_gen = val_idg.flow_from_dataframe(\n",
    "        seed=SEED,\n",
    "        dataframe=df,\n",
    "        directory=None,\n",
    "        x_col='image_path',\n",
    "        y_col='target',\n",
    "        class_mode='raw',\n",
    "        shuffle=False,\n",
    "        target_size=IMG_SIZE,\n",
    "        #batch_size=BATCH_SIZE,\n",
    "        validate_filenames = False\n",
    "    )\n",
    "\n",
    "    return val_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Factory function to create a test image data generator\n",
    "\n",
    "Parameters:\n",
    "    df (dataframe): Test dataframe \n",
    "\n",
    "Returns:\n",
    "    Image Data Generator function\n",
    "\"\"\"\n",
    "def get_test_gen(df):\n",
    "    ## prepare images for validation\n",
    "    test_idg = ImageDataGenerator(rescale=1. / 255.0)\n",
    "    test_gen = test_idg.flow_from_dataframe(\n",
    "        seed=SEED,\n",
    "        dataframe=df,\n",
    "        directory=None,\n",
    "        x_col='image_path',\n",
    "        y_col='target',\n",
    "        class_mode='raw',\n",
    "        shuffle=False,\n",
    "        target_size=IMG_SIZE,\n",
    "        #batch_size=BATCH_SIZE,\n",
    "        validate_filenames = False\n",
    "    )\n",
    "\n",
    "    return test_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67EjcGWPdjaO"
   },
   "source": [
    "## Transfer Learning\n",
    "\n",
    "Conventional machine learning and deep learning algorithms, so far, have been traditionally designed to work in isolation. These algorithms are trained to solve specific tasks. The models have to be rebuilt from scratch once the feature-space distribution changes. Transfer learning is the idea of overcoming the isolated learning paradigm and utilizing knowledge acquired for one task to solve related ones. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fkqvRu-0djaO"
   },
   "source": [
    "\n",
    "![Transfer Learning](https://github.com/SaschaMet/melanoma-classification/blob/master/images/transfer-learning.png?raw=1)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g-vCqePSD4iI"
   },
   "source": [
    "Traditional learning is isolated and occurs purely based on specific tasks, datasets, and training separate isolated models on them. No knowledge is retained, which can be transferred from one model to another. In transfer learning, you can leverage knowledge (features, weights, etc.) from previously trained models for training newer models and even tackle problems like having less data for the more recent task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oLBLoI_mEp3q"
   },
   "source": [
    "**Fine Tuning Off-the-shelf Pre-trained Models**\n",
    "\n",
    "This is a more involved technique, where we do not just replace the final layer (for classification/regression), but we also selectively retrain some of the previous layers. \n",
    "\n",
    "\n",
    "![Transfer Learning](https://miro.medium.com/max/700/1*BBZGHtI_vhDBeqsIbgMj1w.png)\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TXTzfRx8EmSV"
   },
   "source": [
    "Source: https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5aWRaNHUdjaO"
   },
   "outputs": [],
   "source": [
    "\"\"\" Helper function which returns a DenseNet121 model\n",
    "\"\"\"\n",
    "from keras.applications.densenet import DenseNet121\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.densenet import preprocess_input\n",
    "import numpy as np\n",
    "\n",
    "def load_pretrained_model():\n",
    "    base_model = DenseNet121(\n",
    "        input_shape=INPUT_SHAPE,\n",
    "        include_top=False,\n",
    "        weights='imagenet'\n",
    "    )\n",
    "\n",
    "    return base_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4tiogvTddjaO",
    "outputId": "6e60b24b-a310-44b6-d066-eac8e234a19f"
   },
   "outputs": [],
   "source": [
    "model = load_pretrained_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_layer_shape = model.layers[-1].output_shape\n",
    "last_layer_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zxj3bFJPdjaR",
    "outputId": "fbc3976a-1fbb-40fc-90a2-dd521dc63d7f"
   },
   "outputs": [],
   "source": [
    "# create a training and validation dataset from the train df\n",
    "train_df, val_df, test_df = create_splits(train, 0.2, 'target')\n",
    "\n",
    "print(\"rows in train_df\", train_df.shape[0])\n",
    "print(\"rows in val_df\", val_df.shape[0])\n",
    "print(\"rows in test_df\", test_df.shape[0])\n",
    "\n",
    "# call the generator functions\n",
    "#train_gen = get_training_gen(train_df)\n",
    "#val_gen = get_validation_gen(val_df)\n",
    "#test_gen = get_test_gen(test_df)\n",
    "#valX, valY = val_gen.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = train_df.shape[0]\n",
    "val_len = val_df.shape[0]\n",
    "test_len = test_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Helper function for feature extraction\n",
    "\"\"\"\n",
    "\n",
    "def extract_features(df):\n",
    "\n",
    "    features = []\n",
    "    labels = []\n",
    "    for img_path in df['image_path']:\n",
    "        img = image.load_img(img_path, target_size=INPUT_SHAPE)\n",
    "        img_data = image.img_to_array(img)\n",
    "        img_data = np.expand_dims(img_data, axis=0)\n",
    "        img_data = preprocess_input(img_data)\n",
    "\n",
    "        feature = model.predict(img_data)\n",
    "        feature_np = np.array(feature)\n",
    "        features.append(feature_np.flatten())\n",
    "        labels.append(df.loc[df['image_path'] == img_path, 'target'].iloc[0])\n",
    "        \n",
    "    feature_list_np = np.array(features)\n",
    "    labels_list_np = np.array(labels)\n",
    "    \n",
    "    return feature_list_np, labels_list_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_labels = extract_features(train_df)\n",
    "val_features, val_labels = extract_features(val_df)\n",
    "test_features, test_labels = extract_features(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train_features, train_labels\n",
    "X_val, y_val = val_features, val_labels\n",
    "X_test, y_test = test_features, test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier_SVM = SVC(kernel = 'rbf', random_state = 0)\n",
    "classifier_SVM.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc_SVM = classifier_SVM.score(X_train, y_train)\n",
    "val_acc_SVM = classifier_SVM.score(X_val, y_val)\n",
    "test_acc_SVM = classifier_SVM.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_acc_SVM)\n",
    "print(val_acc_SVM)\n",
    "print(test_acc_SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_SVM = classifier_SVM.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "def print_performance_metrics(test_labels,predict):\n",
    "    print('Accuracy:', np.round(metrics.accuracy_score(test_labels, predict),4))\n",
    "    print('ROC Area:', np.round(metrics.roc_auc_score(test_labels, predict),4))\n",
    "    print('Precision:', np.round(metrics.precision_score(test_labels, predict,average='weighted'),4))\n",
    "    print('Recall:', np.round(metrics.recall_score(test_labels, predict,\n",
    "                                               average='weighted'),4))\n",
    "    print('F1 Score:', np.round(metrics.f1_score(test_labels, predict,\n",
    "                                               average='weighted'),4))\n",
    "    print('Cohen Kappa Score:', np.round(metrics.cohen_kappa_score(test_labels, predict),4))\n",
    "    print('Matthews Corrcoef:', np.round(metrics.matthews_corrcoef(test_labels, predict),4)) \n",
    "    print('\\t\\tClassification Report:\\n', metrics.classification_report(test_labels, predict))\n",
    "\n",
    "print_performance_metrics(y_test,y_pred_SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a confusion matrix\n",
    "cm_SVM =  confusion_matrix(y_test,y_pred_SVM)\n",
    "cm_SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Helper function to plot a confusion matrix\n",
    "\n",
    "    Parameters:\n",
    "        cm (confusion matrix)\n",
    "\n",
    "    Returns:\n",
    "        Null\n",
    "\"\"\"\n",
    "def plot_confusion_matrix(cm, labels):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(labels))\n",
    "    plt.xticks(tick_marks, labels, rotation=55)\n",
    "    plt.yticks(tick_marks, labels)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], 'd'), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "cm_plot_label =['benign', 'malignant']\n",
    "plot_confusion_matrix(cm_SVM, cm_plot_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier_RF = RandomForestClassifier(n_estimators = 800, criterion = 'entropy', random_state = 0)\n",
    "classifier_RF.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc_RF = classifier_RF.score(X_train, y_train)\n",
    "val_acc_RF = classifier_RF.score(X_val, y_val)\n",
    "test_acc_RF = classifier_RF.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_acc_RF)\n",
    "print(val_acc_RF)\n",
    "print(test_acc_RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_RF = classifier_RF.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_performance_metrics(y_test,y_pred_RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a confusion matrix\n",
    "cm_RF =  confusion_matrix(y_test,y_pred_RF)\n",
    "cm_RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm_RF, cm_plot_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADABOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "classifier_AdaBoost = AdaBoostClassifier(n_estimators = 100)\n",
    "classifier_AdaBoost.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc_AdaBoost = classifier_AdaBoost.score(X_train, y_train)\n",
    "val_acc_AdaBoost = classifier_AdaBoost.score(X_val, y_val)\n",
    "test_acc_AdaBoost = classifier_AdaBoost.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_acc_AdaBoost)\n",
    "print(val_acc_AdaBoost)\n",
    "print(test_acc_AdaBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_AdaBoost = classifier_AdaBoost.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_performance_metrics(y_test,y_pred_AdaBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a confusion matrix\n",
    "cm_AdaBoost =  confusion_matrix(y_test,y_pred_AdaBoost)\n",
    "cm_AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm_AdaBoost, cm_plot_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier_kNN = KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30)\n",
    "classifier_kNN.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc_kNN = classifier_kNN.score(X_train, y_train)\n",
    "val_acc_kNN = classifier_kNN.score(X_val, y_val)\n",
    "test_acc_kNN = classifier_kNN.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_acc_kNN)\n",
    "print(val_acc_kNN)\n",
    "print(test_acc_kNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_kNN = classifier_kNN.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_performance_metrics(y_test,y_pred_kNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a confusion matrix\n",
    "cm_kNN =  confusion_matrix(y_test,y_pred_kNN)\n",
    "cm_kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm_kNN, cm_plot_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "classifier_xgb = xgb.XGBClassifier(n_estimators = 300)\n",
    "classifier_xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc_xgb = classifier_xgb.score(X_train, y_train)\n",
    "val_acc_xgb = classifier_xgb.score(X_val, y_val)\n",
    "test_acc_xgb = classifier_xgb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_acc_xgb)\n",
    "print(val_acc_xgb)\n",
    "print(test_acc_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_xgb = classifier_xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_performance_metrics(y_test,y_pred_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a confusion matrix\n",
    "cm_xgb =  confusion_matrix(y_test,y_pred_xgb)\n",
    "cm_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm_xgb, cm_plot_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BAGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "classifier_Bagging = BaggingClassifier(n_estimators=100)\n",
    "classifier_Bagging.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc_Bagging = classifier_Bagging.score(X_train, y_train)\n",
    "val_acc_Bagging = classifier_Bagging.score(X_val, y_val)\n",
    "test_acc_Bagging = classifier_Bagging.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_acc_Bagging)\n",
    "print(val_acc_Bagging)\n",
    "print(test_acc_Bagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_Bagging = classifier_Bagging.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_performance_metrics(y_test,y_pred_Bagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a confusion matrix\n",
    "cm_Bagging =  confusion_matrix(y_test,y_pred_Bagging)\n",
    "cm_Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm_Bagging, cm_plot_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ANN = tf.keras.models.Sequential([\n",
    "    tf.keras.Input(shape=(last_layer_shape[1],last_layer_shape[2],last_layer_shape[3])),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    #tf.keras.layers.Dense(64, activation = 'relu'),\n",
    "    #tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "LEARNING_RATE = 1e-4\n",
    "OPTIMIZER = RMSprop(lr=LEARNING_RATE,decay=1e-2)\n",
    "LOSS = 'binary_crossentropy'\n",
    "METRICS = [\n",
    "    'accuracy', \n",
    "    'AUC'\n",
    "] \n",
    "\n",
    "model_ANN.compile(\n",
    "    loss=LOSS,\n",
    "    metrics=METRICS,\n",
    "    optimizer=OPTIMIZER,\n",
    ")\n",
    "\n",
    "print(\"fit model on gpu\")\n",
    "history_ANN = model_ANN.fit(\n",
    "    train_features, train_labels, \n",
    "    epochs=EPOCHS, \n",
    "    verbose=VERBOSE_LEVEL, \n",
    "    validation_data=(val_features,val_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the current timestamp. This timestamp is used to save the model data with a unique name\n",
    "now = datetime.now()\n",
    "today = date.today()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "timestamp = str(today) + \"_\" + str(current_time)\n",
    "\n",
    "# plot model history\n",
    "save_history(history_ANN.history, timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_ANN = model_ANN.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Helper function turn the model predictions into a binary (0,1) format\n",
    "\n",
    "    Parameters:\n",
    "        pred (float): Model prediction\n",
    "\n",
    "    Returns:\n",
    "        binary prediction (int)\n",
    "\"\"\"\n",
    "\n",
    "def pred_to_binary(pred):\n",
    "    if pred < 0.5:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "y_pred_ANN = [pred_to_binary(x) for x in y_pred_ANN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_performance_metrics(y_test,y_pred_ANN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a confusion matrix\n",
    "cm_ANN =  confusion_matrix(y_test,y_pred_ANN)\n",
    "cm_ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm_ANN, cm_plot_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For LSTMs\n",
    "\n",
    "train_features_2d = np.zeros((train_len,last_layer_shape[1],last_layer_shape[2]*last_layer_shape[3]))\n",
    "for i in range(len(train_labels)):\n",
    "    train_features_2d[i] = train_features[i].reshape(last_layer_shape[1],\n",
    "                                                     last_layer_shape[2]*last_layer_shape[3])\n",
    "    \n",
    "val_features_2d = np.zeros((val_len,last_layer_shape[1],last_layer_shape[2]*last_layer_shape[3]))\n",
    "for i in range(len(val_labels)):\n",
    "    val_features_2d[i] = val_features[i].reshape(last_layer_shape[1],\n",
    "                                                     last_layer_shape[2]*last_layer_shape[3])\n",
    "    \n",
    "test_features_2d = np.zeros((test_len,last_layer_shape[1],last_layer_shape[2]*last_layer_shape[3]))\n",
    "for i in range(len(test_labels)):\n",
    "    test_features_2d[i] = test_features[i].reshape(last_layer_shape[1],\n",
    "                                                     last_layer_shape[2]*last_layer_shape[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LSTM = tf.keras.models.Sequential([\n",
    "    tf.keras.Input(shape=(last_layer_shape[1], last_layer_shape[2]*last_layer_shape[3])),\n",
    "    tf.keras.layers.LSTM(100, return_sequences=True),\n",
    "    #tf.keras.layers.LSTM(32, return_sequences=True),\n",
    "    tf.keras.layers.LSTM(32),\n",
    "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "LEARNING_RATE = 1e-4\n",
    "OPTIMIZER = Adam(lr=LEARNING_RATE,decay=1e-2)\n",
    "LOSS = 'binary_crossentropy'\n",
    "METRICS = [\n",
    "    'accuracy', \n",
    "    'AUC'\n",
    "] \n",
    "\n",
    "model_LSTM.compile(\n",
    "    loss=LOSS,\n",
    "    metrics=METRICS,\n",
    "    optimizer=OPTIMIZER,\n",
    ")\n",
    "\n",
    "print(\"fit model on gpu\")\n",
    "history_LSTM = model_LSTM.fit(\n",
    "    train_features_2d, train_labels, \n",
    "    epochs=EPOCHS, \n",
    "    verbose=VERBOSE_LEVEL,\n",
    "    validation_data=(val_features_2d,val_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot model history\n",
    "save_history(history_LSTM.history, timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_LSTM = model_LSTM.predict(test_features_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_LSTM = [pred_to_binary(x) for x in y_pred_LSTM]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_performance_metrics(y_test,y_pred_LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a confusion matrix\n",
    "cm_LSTM =  confusion_matrix(y_test,y_pred_LSTM)\n",
    "cm_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm_LSTM, cm_plot_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIDIRECTIONAL LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Bi_LSTM = tf.keras.models.Sequential([\n",
    "    tf.keras.Input(shape=(last_layer_shape[1], last_layer_shape[2]*last_layer_shape[3])),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(100, return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(100)),\n",
    "    #tf.keras.layers.Dropout(0.3),\n",
    "    #tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128)),\n",
    "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "LEARNING_RATE = 1e-4\n",
    "OPTIMIZER = Adam(lr=LEARNING_RATE,decay=1e-2)\n",
    "LOSS = 'binary_crossentropy'\n",
    "METRICS = [\n",
    "    'accuracy', \n",
    "    'AUC'\n",
    "] \n",
    "\n",
    "model_Bi_LSTM.compile(\n",
    "    loss=LOSS,\n",
    "    metrics=METRICS,\n",
    "    optimizer=OPTIMIZER,\n",
    ")\n",
    "\n",
    "print(\"fit model on gpu\")\n",
    "history_Bi_LSTM = model_Bi_LSTM.fit(\n",
    "    train_features_2d, train_labels, \n",
    "    epochs=EPOCHS, \n",
    "    verbose=VERBOSE_LEVEL, \n",
    "    validation_data=(val_features_2d,val_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot model history\n",
    "save_history(history_Bi_LSTM.history, timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_Bi_LSTM = model_Bi_LSTM.predict(test_features_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_Bi_LSTM = [pred_to_binary(x) for x in y_pred_Bi_LSTM]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_performance_metrics(y_test,y_pred_Bi_LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a confusion matrix\n",
    "cm_Bi_LSTM =  confusion_matrix(y_test,y_pred_Bi_LSTM)\n",
    "cm_Bi_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm_Bi_LSTM, cm_plot_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
